import time
import streamlit as st
import logging
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_community.llms import Ollama
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains.combine_documents import create_stuff_documents_chain

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)


faiss_index_path = "db"
embedding_model_name = "jhgan/ko-sroberta-multitask"
llm_model_name = "EEVE-Korean-10.8B-Q5_K_M-GGUF"


@st.cache_resource
def load_embedding_model(model_name):
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)
    return embedding_model


@st.cache_resource
def load_vectorstore(index_path, _embedding_model):
    vectorstore = FAISS.load_local(
        folder_path=index_path,
        embeddings=_embedding_model,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


@st.cache_resource
def load_llm(llm_model_name):
    llm = Ollama(model=llm_model_name)
    return llm


@st.cache_resource
def create_rag_chain(embedding_model_name, faiss_index_path, llm_model_name):
    logging.info("ì„œë²„ ì‹œìž‘í•©ë‹ˆë‹¤.")

    embedding_model = load_embedding_model(embedding_model_name)
    logging.info("ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    vectorstore = load_vectorstore(faiss_index_path, embedding_model)
    logging.info("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    llm = load_llm(llm_model_name)
    logging.info("LLM ë¡œë“œ ì™„ë£Œ")
    retriever = vectorstore.as_retriever(
        search_type="similarity", search_kwargs={"k": 2}
    )
    question_rephrasing_chain = create_question_rephrasing_chain(llm, retriever)
    question_answering_chain = create_question_answering_chain(llm)
    rag_chain = create_retrieval_chain(
        question_rephrasing_chain, question_answering_chain
    )
    logging.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return rag_chain


def create_question_rephrasing_chain(llm, retriever):
    system_prompt = """
    ë‹¹ì‹ ì€ ì§ˆë¬¸ ìž¬êµ¬ì„±ìžìž…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ìž ì§ˆë¬¸ì´ ìžˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìžˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìžˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
    ì´ ìž¬êµ¬ì„±ëœ ì§ˆë¬¸ì€ ë¬¸ì„œ ê²€ìƒ‰ì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì‚¬ìš©ìžì—ê²Œ ì œê³µí•  ìµœì¢… ë‹µë³€ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”. ì ˆëŒ€ ì§ˆë¬¸ì— ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
    
    ì˜ˆì‹œ:
    ê´€ë ¨ ìžˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ì„ ë°±ì—…í•˜ê³  ì‹¶ì–´
    AI: ë©”ì¼ ë°±ì—…ì€ ê¸°ë³¸ë©”ì¼í•¨ ê´€ë¦¬ > ë‚´ ë©”ì¼í•¨ ê´€ë¦¬ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì´ìš©í•´ ë©”ì¼í•¨ì„ zip íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ê¸°ê°„ì˜ ë©”ì¼ì„ ë°±ì—…í•˜ë ¤ë©´, ê¸°ê°„ë³„ ë°±ì—…ì„ ì²´í¬í•˜ì„¸ìš”. ë°±ì—…í•œ ë©”ì¼ì€ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì‹œ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    Human: ì—…ë¡œë“œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ë°±ì—…í•œ ë©”ì¼ì„ ì—…ë¡œë“œí•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
    
    ê´€ë ¨ ìžˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìžˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: í˜•ì‹ì— ì œí•œì´ ìžˆë‚˜ìš”?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í˜•ì‹ ì œí•œì´ ìžˆë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìžˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ì¼ì • ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜
    AI: ì¼ì • ë“±ë¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‚ ì§œë¥¼ ì„ íƒí•´ ë“±ë¡í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¼ì‹œë¥¼ ì •í•œ í›„, ìº˜ë¦°ë”ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ê³ , ì•ŒëžŒì„ í†µí•´ ë¯¸ë¦¬ ì¼ì •ì„ ì•Œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    return create_history_aware_retriever(llm, retriever, prompt)


def create_question_answering_chain(llm):
    system_prompt = """ë‹¹ì‹ ì€ í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ìž…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    
    ì˜ˆì‹œ:
    ðŸ“ì‚¬ìš©ìž ì§ˆë¬¸: í•œë²ˆì— ì—…ë¡œë“œ ê°€ëŠ¥í•œ íŒŒì¼ ê°¯ìˆ˜ëŠ” ëª‡ê°œì¸ê°€ìš”?
    ðŸ“ë‹µë³€: í•œë²ˆì— ì—…ë¡œë“œ ê°€ëŠ¥í•œ ê°¯ìˆ˜ê°€ ì •í•´ì ¸ìžˆì§€ ì•Šì§€ë§Œ, ì¼ë°˜ ì²¨ë¶€ê°™ì€ê²½ìš° 20MB ,ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2048MBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    ðŸ“ì‚¬ìš©ìž ì§ˆë¬¸: í•´ì™¸ì—ì„œ ë©”ì¼ ì‚¬ìš©ì´ ê°€ëŠ¥í•œê°€ìš”?
    ðŸ“ë‹µë³€: í™˜ê²½ì„¤ì • - ê°œì¸ì •ë³´/ë³´ì•ˆ ê¸°ëŠ¥ - ë³´ì•ˆ ì„¤ì •ì—ì„œ êµ­ê°€ë³„ ë¡œê·¸ì¸ í—ˆìš© ê¸°ëŠ¥ì„ ì´ìš©í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ë³´ì•ˆ ì„¤ì •íƒ­ì´ ë³´ì´ì§€ ì•Šì„ ì‹œì— ë©”ì¼ ë‹´ë‹¹ìžì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. 
    
    ðŸ“ì‚¬ìš©ìž ì§ˆë¬¸: ì—¬ëŸ¬ëª…ì—ê²Œ ê°œë³„ ë°œì†¡í•˜ê³  ì‹¶ì–´ìš”
    ðŸ“ë‹µë³€: ë©”ì¼ ê°œë³„ë°œì†¡ ì„¤ì •ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê°œë³„ë°œì†¡ì´ëž€ ì—¬ëŸ¬ ì‚¬ëžŒì—ê²Œ ë™ì‹œì— ë©”ì¼ì„ ë³´ë‚´ë„ ë°›ëŠ”ì‚¬ëžŒ ì˜ì—­ì— ìˆ˜ì‹ ì¸ ë³¸ì¸ í•œ ëª…ë§Œ í‘œì‹œë˜ëŠ” ê¸°ëŠ¥ìž…ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” ì„¤ì •ë˜ì–´ìžˆì§€ ì•Šì§€ë§Œ, ë©”ì¼ì“°ê¸° íƒ­ì˜ ë³´ë‚´ê¸° ì„¤ì •ì—ì„œ í•œëª…ì”© ë°œì†¡ì„ ì²´í¬í•˜ì‹œë©´ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

    
    ## ê²€ìƒ‰ëœ ë¬¸ì„œìž…ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” ë¹ˆì¤„ë¡œ êµ¬ë¶„ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.
    {context}
    
    ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì„¸ ë¬¸ìž¥ ì´ë‚´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ëª¨ë¥¸ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”. ì˜ˆì‹œì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}\n"),
        ]
    )

    return create_stuff_documents_chain(llm, prompt)


def clean_data(data):
    cleaned_data = []
    for item in data:
        cleaned_page_content = item.page_content.replace("\n", " ").strip()
        cleaned_data.append({"page_content": cleaned_page_content})
    return cleaned_data


def reset_chat():
    st.session_state.messages = []


def main():
    st.title("RAG Chatbot")

    # RAG ì²´ì¸ ìƒì„±
    rag_chain = create_rag_chain(embedding_model_name, faiss_index_path, llm_model_name)
    st.session_state.rag_chain = rag_chain

    # ë©”ì„¸ì§€ê°€ ì—†ë‹¤ë©´ []ìœ¼ë¡œ ì„ ì–¸
    if "messages" not in st.session_state:
        st.session_state.messages = []

    #
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_chat()
        st.toast("ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âŒ")

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Ask a question!"):
        MAX_MESSAGES_BEFORE_DELETION = 4

        if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
            del st.session_state.messages[:2]

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            rag_chain = st.session_state.rag_chain
            result = rag_chain.invoke(
                {"input": prompt, "chat_history": st.session_state.messages}
            )

            st.session_state.messages.append({"role": "user", "content": prompt})

            cleaned_datas = clean_data(result["context"])

            for cleaned_data in cleaned_datas:
                with st.expander("Evidence context"):
                    st.write(f"Page content: {cleaned_data['page_content']}")

            for chunk in result["answer"].split(" "):
                full_response += chunk + " "
                time.sleep(0.2)
                message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

        st.session_state.messages.append(
            {"role": "assistant", "content": full_response}
        )


main()
