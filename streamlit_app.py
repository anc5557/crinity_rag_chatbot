import time
import streamlit as st
import logging
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_community.llms import Ollama
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains.combine_documents import create_stuff_documents_chain

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)


# LLM íƒ€ìž… ì„¤ì •
LLM_TYPE = "Ollama"  # "HuggingFace" ë˜ëŠ” "Ollama"

faiss_index_path = "db"
embedding_model_name = "jhgan/ko-sroberta-multitask"
huggingface_llm_model_name = "beomi/gemma-ko-2b"  # beomi/gemma-ko-2b
ollama_llm_model_name = "llama-3-Korean-Bllossom-8B-gguf-Q4_K_M"  # llama-3-Korean-Bllossom-8B-gguf-Q4_K_M or EEVE-Korean-10.8B-Q5_K_M-GGUF or EEVE-Korean-Instruct-10.8B-v1.0-GGUF-Q4-K-M


@st.cache_resource
def load_embedding_model(model_name):
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name, encode_kwargs={"normalize_embeddings": True}
    )
    return embedding_model


@st.cache_resource
def load_vectorstore(index_path, _embedding_model):
    vectorstore = FAISS.load_local(
        folder_path=index_path,
        embeddings=_embedding_model,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


@st.cache_resource
def load_llm(llm_model_name, llm_type):
    if llm_type == "HuggingFace":
        llm = HuggingFacePipeline.from_model_id(
            model_id=llm_model_name,
            device=0,
            task="text-generation",
            pipeline_kwargs={
                "max_length": 1000,
                "num_return_sequences": 1,
                "max_new_tokens": 500,
            },
        )
    elif llm_type == "Ollama":
        llm = Ollama(model=llm_model_name)
    return llm


@st.cache_resource
def create_rag_chain(embedding_model_name, faiss_index_path, llm_model_name, llm_type):
    logging.info("ì„œë²„ ì‹œìž‘í•©ë‹ˆë‹¤.")
    embedding_model = load_embedding_model(embedding_model_name)
    logging.info("ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    vectorstore = load_vectorstore(faiss_index_path, embedding_model)
    logging.info("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    llm = load_llm(llm_model_name, llm_type)
    logging.info("LLM ë¡œë“œ ì™„ë£Œ")

    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"score_threshold": 0.5, "k": 3},
        verbose=True,
    )
    question_rephrasing_chain = create_question_rephrasing_chain(llm, retriever)
    question_answering_chain = create_question_answering_chain(llm)
    rag_chain = create_retrieval_chain(
        question_rephrasing_chain, question_answering_chain
    )
    logging.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return rag_chain


def create_question_rephrasing_chain(llm, retriever):
    system_prompt = """
    ë‹¹ì‹ ì€ ì§ˆë¬¸ ìž¬êµ¬ì„±ìžìž…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ìž ì§ˆë¬¸ì´ ìžˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìžˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìžˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. 
    ì´ ìž¬êµ¬ì„±ëœ ì§ˆë¬¸ì€ ë¬¸ì„œ ê²€ìƒ‰ì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì‚¬ìš©ìžì—ê²Œ ì œê³µí•  ìµœì¢… ë‹µë³€ì—ëŠ” ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”. ì ˆëŒ€ ì§ˆë¬¸ì— ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
    
    ì˜ˆì‹œ:
    ê´€ë ¨ ìžˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ì„ ë°±ì—…í•˜ê³  ì‹¶ì–´
    AI: ë©”ì¼ ë°±ì—…ì€ ê¸°ë³¸ë©”ì¼í•¨ ê´€ë¦¬ > ë‚´ ë©”ì¼í•¨ ê´€ë¦¬ì—ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì´ìš©í•´ ë©”ì¼í•¨ì„ zip íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ê¸°ê°„ì˜ ë©”ì¼ì„ ë°±ì—…í•˜ë ¤ë©´, ê¸°ê°„ë³„ ë°±ì—…ì„ ì²´í¬í•˜ì„¸ìš”. ë°±ì—…í•œ ë©”ì¼ì€ ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë‹¤ì‹œ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    Human: ì—…ë¡œë“œëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ë°±ì—…í•œ ë©”ì¼ì„ ì—…ë¡œë“œí•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
    
    ê´€ë ¨ ìžˆëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìžˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: í˜•ì‹ì— ì œí•œì´ ìžˆë‚˜ìš”?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í˜•ì‹ ì œí•œì´ ìžˆë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìžˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    Human: ì¼ì • ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜
    AI: ì¼ì • ë“±ë¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‚ ì§œë¥¼ ì„ íƒí•´ ë“±ë¡í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¼ì‹œë¥¼ ì •í•œ í›„, ìº˜ë¦°ë”ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ê³ , ì•ŒëžŒì„ í†µí•´ ë¯¸ë¦¬ ì¼ì •ì„ ì•Œë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    return create_history_aware_retriever(llm, retriever, prompt)


def create_question_answering_chain(llm):
    system_prompt = """ë‹¹ì‹ ì€ í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ìž…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
    ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì„¸ ë¬¸ìž¥ ì´ë‚´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì‹ ë¢°ë„ ë§í•˜ì§€ ë§ˆì„¸ìš”.
    ëª¨ë¥¸ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.
    ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    ì˜ˆì‹œì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ë©´ ì•ˆë©ë‹ˆë‹¤.
    
    ## ê²€ìƒ‰ëœ ë¬¸ì„œìž…ë‹ˆë‹¤. ê° ë¬¸ì„œëŠ” ë¹ˆì¤„ë¡œ êµ¬ë¶„ë˜ì–´ ìžˆìŠµë‹ˆë‹¤.
    {context}
    
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}\n"),
        ]
    )

    return create_stuff_documents_chain(llm, prompt)


def clean_data(data):
    cleaned_data = []
    for item in data:
        cleaned_page_content = item.page_content.replace("\n", " ").strip()
        cleaned_data.append({"page_content": cleaned_page_content})
    return cleaned_data


def reset_chat():
    st.session_state.messages = []


def main():
    st.title("í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡")

    if LLM_TYPE == "HuggingFace":
        llm_model_name = huggingface_llm_model_name
    else:
        llm_model_name = ollama_llm_model_name

    # RAG ì²´ì¸ ìƒì„±
    rag_chain = create_rag_chain(
        embedding_model_name, faiss_index_path, llm_model_name, LLM_TYPE
    )
    st.session_state.rag_chain = rag_chain

    # ë©”ì„¸ì§€ê°€ ì—†ë‹¤ë©´ []ìœ¼ë¡œ ì„ ì–¸
    if "messages" not in st.session_state:
        st.session_state.messages = []

    #
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_chat()
        st.toast("ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âŒ")

    with st.chat_message("assistant"):
        st.markdown("ì•ˆë…•í•˜ì„¸ìš”! í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ìž…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”. ðŸ¤–")

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Ask a question!"):
        MAX_MESSAGES_BEFORE_DELETION = 4

        if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
            del st.session_state.messages[:2]

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            rag_chain = st.session_state.rag_chain
            result = rag_chain.invoke(
                {"input": prompt, "chat_history": st.session_state.messages}
            )

            st.session_state.messages.append({"role": "user", "content": prompt})

            cleaned_datas = clean_data(result["context"])

            for cleaned_data in cleaned_datas:
                with st.expander("Evidence context"):
                    st.write(f"Page content: {cleaned_data['page_content']}")

            for chunk in result["answer"].split(" "):
                full_response += chunk + " "
                time.sleep(0.2)
                message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

        st.session_state.messages.append(
            {"role": "assistant", "content": full_response}
        )


main()
