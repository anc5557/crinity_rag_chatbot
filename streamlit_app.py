import os
import time
import streamlit as st
import logging
from dotenv import load_dotenv
from input_gspread import input_faq
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_community.llms import Ollama
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
)
from langchain.chains.combine_documents import create_stuff_documents_chain

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ LLM íƒ€ì… ë° Ollama URL ì„¤ì •
ENV = os.getenv("ENV", "dev")

LLM_TYPE = "Ollama"  # "HuggingFace" ë˜ëŠ” "Ollama"

# Ollama URL ì„¤ì •
if ENV == "dev":
    OLLAMA_BASE_URL = "http://localhost:11434"
elif ENV == "prod":
    OLLAMA_BASE_URL = "http://ollama:11434"

faiss_index_path = "db"
embedding_model_name = "jhgan/ko-sroberta-multitask"
huggingface_llm_model_name = "beomi/gemma-ko-2b"  # beomi/gemma-ko-2b
ollama_llm_model_name = "EEVE-Korean-Instruct-10.8B-v1.0-GGUF-Q4-K-M"  # llama-3-Korean-Bllossom-8B-gguf-Q4_K_M or EEVE-Korean-10.8B-Q5_K_M-GGUF or EEVE-Korean-Instruct-10.8B-v1.0-GGUF-Q4-K-M


@st.cache_resource
def load_embedding_model(model_name):
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
    embedding_model = HuggingFaceEmbeddings(
        model_name=model_name, encode_kwargs={"normalize_embeddings": True}
    )
    return embedding_model


@st.cache_resource
def load_vectorstore(index_path, _embedding_model):
    """ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
    vectorstore = FAISS.load_local(
        folder_path=index_path,
        embeddings=_embedding_model,
        allow_dangerous_deserialization=True,
    )
    return vectorstore


@st.cache_resource
def load_llm(llm_model_name, llm_type):
    """LLM ë¡œë“œ
    llm_type: HuggingFace ë˜ëŠ” Ollama
    """
    if llm_type == "HuggingFace":
        llm = HuggingFacePipeline.from_model_id(
            model_id=llm_model_name,
            device=0,
            task="text-generation",
            pipeline_kwargs={
                "max_length": 1000,
                "num_return_sequences": 1,
                "max_new_tokens": 500,
            },
        )
    elif llm_type == "Ollama":
        llm = Ollama(model=llm_model_name, base_url=OLLAMA_BASE_URL)
    return llm


@st.cache_resource
def create_rag_chain(embedding_model_name, faiss_index_path, llm_model_name, llm_type):
    """RAG ì²´ì¸ ìƒì„±"""

    logging.info("ì„œë²„ ì‹œì‘í•©ë‹ˆë‹¤.")
    embedding_model = load_embedding_model(embedding_model_name)
    logging.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    vectorstore = load_vectorstore(faiss_index_path, embedding_model)
    logging.info("ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ")
    llm = load_llm(llm_model_name, llm_type)
    logging.info("LLM ë¡œë“œ ì™„ë£Œ")

    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"score_threshold": 0.5, "k": 3},
        verbose=True,
    )
    question_rephrasing_chain = create_question_rephrasing_chain(llm, retriever)
    question_answering_chain = create_question_answering_chain(llm)
    rag_chain = create_retrieval_chain(
        question_rephrasing_chain, question_answering_chain
    )
    logging.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return rag_chain


def create_question_rephrasing_chain(llm, retriever):
    """ì§ˆë¬¸ ì¬êµ¬ì„± ì²´ì¸ ìƒì„±"""

    system_prompt = """
    ë‹¹ì‹ ì€ ì§ˆë¬¸ ì¬êµ¬ì„±ìì…ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    ê´€ë ¨ì´ ìˆëŠ” ê²½ìš°, ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì„¸ìš”. 
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°, ì‚¬ìš©ì ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•´ì£¼ì„¸ìš”.
    
    ì˜ˆì‹œ:
    ê´€ë ¨ ìˆëŠ” ê²½ìš°)
    1.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: í˜•ì‹ì— ì œí•œì´ ìˆë‚˜ìš”?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í˜•ì‹ ì œí•œì´ ìˆë‚˜ìš”?
    
    2.
    Human: ì¼ì • ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜
    AI: ì¼ì • ë“±ë¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‚ ì§œë¥¼ ì„ íƒí•´ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¼ì‹œë¥¼ ì •í•œ í›„, ìº˜ë¦°ë”ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ê³ , ì•ŒëŒì„ í†µí•´ ë¯¸ë¦¬ ì¼ì •ì„ ì•Œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Human: ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì€?
    ë‹µë³€: ì¼ì • ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì€?
    
    3.
    Human: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    AI: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´, ì£¼ì†Œë¡ì—ì„œ ì£¼ì†Œë¥¼ ì„ íƒí•œ í›„, ì´ë™ ë˜ëŠ” ë³µì‚¬ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”. ì´ë™í•  ì£¼ì†Œë¡ì„ ì„ íƒí•˜ê±°ë‚˜, ìƒˆë¡œìš´ ì£¼ì†Œë¡ì„ ë§Œë“¤ì–´ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Human: ë³µì‚¬í•˜ë©´ ë¶™ì—¬ë„£ê¸°ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ì£¼ì†Œë¡ì— ë³µì‚¬í•œ ì£¼ì†Œë¥¼ ë¶™ì—¬ë„£ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
    
    ê´€ë ¨ ì—†ëŠ” ê²½ìš°)
    1.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    AI: ì¼ë°˜ ì²¨ë¶€íŒŒì¼ì˜ ê²½ìš° 20MB, ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²¨ë¶€ì˜ ê²½ìš° 2GBê¹Œì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    Human: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    ë‹µë³€: ì£¼ì†Œë¡ì— ì£¼ì†Œë¥¼ ì´ë™/ë³µì‚¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?
    
    2.
    Human: ì¼ì • ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì¤˜
    AI: ì¼ì • ë“±ë¡ ë²„íŠ¼ì„ ëˆ„ë¥´ê±°ë‚˜ ë‚ ì§œë¥¼ ì„ íƒí•´ ë“±ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œëª©ê³¼ ì¼ì‹œë¥¼ ì •í•œ í›„, ìº˜ë¦°ë”ì˜ ì¢…ë¥˜ë¥¼ ì„ íƒí•˜ê³ , ì•ŒëŒì„ í†µí•´ ë¯¸ë¦¬ ì¼ì •ì„ ì•Œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì€?
    
    3.
    Human: ì•ˆë…•
    AI: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?
    Human: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    ë‹µë³€: ë©”ì¼ ì²¨ë¶€íŒŒì¼ í¬ê¸° ì œí•œì´ ìˆë‚˜ìš”?
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    return create_history_aware_retriever(llm, retriever, prompt)


def create_question_answering_chain(llm):
    """ì§ˆë¬¸ ë‹µë³€ ì²´ì¸ ìƒì„±"""

    system_prompt = """ë‹¹ì‹ ì€ í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.ëª¨ë¥¸ë‹¤ë©´, ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”. ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš° "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.
    ## ê²€ìƒ‰ëœ ë¬¸ì„œ ##
    {context}
    ################
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}\n"),
        ]
    )

    return create_stuff_documents_chain(llm, prompt)


def clean_data(data):
    """ë°ì´í„° ì •ì œ"""
    cleaned_data = []
    for item in data:
        cleaned_page_content = item.page_content.replace("\n", " ").strip()
        cleaned_data.append({"page_content": cleaned_page_content})
    return cleaned_data


def reset_chat():
    st.session_state.messages = []


def main():
    st.title("ğŸ’­í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡")

    with st.expander("ì•Œë¦¼", icon="ğŸ“¢", expanded=True):
        """ì‚¬ë‚´ í…ŒìŠ¤íŠ¸ ì¤‘ì…ë‹ˆë‹¤. ë¬¸ì„œëŠ” cm9.3 ì‚¬ìš©ì ë©”ë‰´ì–¼ì…ë‹ˆë‹¤. \n\n ë¬¸ì„œ ê°œì„  ì‘ì—…ì¤‘ìœ¼ë¡œ ë‚´ìš©ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n\n ë‹µë³€ì´ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](https://docs.google.com/spreadsheets/d/1iu9H_OZPtnvGGXM07axeCWz8sH5eSSOpi84nrzMX5B0/edit?pli=1&gid=0#gid=0)ë¥¼ í´ë¦­í•˜ì—¬ 'ìƒì´í•¨'ì— ì²´í¬í•´ì£¼ì„¸ìš”."""

    if LLM_TYPE == "HuggingFace":
        llm_model_name = huggingface_llm_model_name
    else:
        llm_model_name = ollama_llm_model_name

    # RAG ì²´ì¸ ìƒì„±
    rag_chain = create_rag_chain(
        embedding_model_name, faiss_index_path, llm_model_name, LLM_TYPE
    )
    st.session_state.rag_chain = rag_chain

    # ë©”ì„¸ì§€ê°€ ì—†ë‹¤ë©´ []ìœ¼ë¡œ ì„ ì–¸
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        reset_chat()
        st.toast("ì´ˆê¸°í™” ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="âŒ")

    with st.chat_message("assistant"):
        st.markdown(
            "ì•ˆë…•í•˜ì„¸ìš”! í¬ë¦¬ë‹ˆí‹° Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ¤– \n\n "
        )
        st.markdown("")

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Ask a question!"):
        MAX_MESSAGES_BEFORE_DELETION = 4  # ìµœëŒ€ ë©”ì„¸ì§€ ìˆ˜

        if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
            del st.session_state.messages[:2]

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            with st.spinner("ë‹µë³€ì¤‘ì…ë‹ˆë‹¤..."):
                rag_chain = st.session_state.rag_chain
                result = rag_chain.invoke(
                    {"input": prompt, "chat_history": st.session_state.messages}
                )

                st.session_state.messages.append({"role": "user", "content": prompt})

                cleaned_datas = clean_data(result["context"])

                for i, cleaned_data in enumerate(cleaned_datas):
                    with st.expander(f"ì°¸ê³  ë¬¸ì„œ {i+1}"):
                        st.write(cleaned_data["page_content"])

                for chunk in result["answer"].split(" "):
                    full_response += chunk + " "
                    time.sleep(0.2)
                    message_placeholder.markdown(full_response + "â–Œ")
                    message_placeholder.markdown(full_response)

            st.session_state.messages.append(
                {"role": "assistant", "content": full_response}
            )

            # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì— ê¸°ë¡
            if ENV == "prod":
                input_faq(prompt, full_response.strip())


main()
